fastapi
uvicorn
celery
redis
python-dotenv
python-multipart
aiofiles
openai-whisper      # NEW - OpenAI's Whisper
torch               # NEW - PyTorch for Whisper
torchaudio          # NEW - Audio processing
numpy               # NEW - Numerical operations
import whisper
import numpy as np
import os
from datetime import timedelta

class WhisperProcessor:
    def __init__(self, model_size="base"):
        """
        Initialize Whisper model
        model_size: "tiny", "base", "small", "medium", "large"
        """
        print(f"Loading Whisper {model_size} model...")
        self.model = whisper.load_model(model_size)
        print("Whisper loaded successfully!")
    
    def transcribe(self, video_path):
        """Transcribe video and return segments with timestamps"""
        result = self.model.transcribe(video_path)
        return result
    
    def extract_highlights(self, video_path, min_duration=60, max_duration=120):
        """
        Find the most engaging segments based on:
        - Speech patterns
        - Keyword density
        - Question detection
        - Excitement in voice (via duration/patterns)
        """
        result = self.transcribe(video_path)
        segments = result["segments"]
        
        # Score each segment
        scored_segments = []
        for seg in segments:
            score = 0
            text = seg["text"].lower()
            duration = seg["end"] - seg["start"]
            
            # Skip very short segments
            if duration < 5:
                continue
            
            # Score based on keywords
            keywords = ["amazing", "wow", "important", "key", "secret", 
                       "breakthrough", "incredible", "game changer"]
            for kw in keywords:
                if kw in text:
                    score += 10
            
            # Score questions (engaging content)
            if "?" in text:
                score += 15
            
            # Score based on exclamation/emphasis
            if "!" in text:
                score += 10
            
            # Score based on segment length (longer = more substantial)
            score += min(duration / 10, 20)
            
            scored_segments.append({
                "start": seg["start"],
                "end": seg["end"],
                "text": seg["text"],
                "score": score,
                "duration": duration
            })
        
        # Sort by score and merge adjacent segments
        scored_segments.sort(key=lambda x: x["score"], reverse=True)
        
        # Build clips of desired duration
        clips = []
        used_times = set()
        
        for seg in scored_segments:
            # Check if this time range is already used
            time_range = range(int(seg["start"]), int(seg["end"]) + 1)
            if any(t in used_times for t in time_range):
                continue
            
            # Find adjacent high-scoring segments to build longer clip
            clip_start = seg["start"]
            clip_end = seg["end"]
            clip_texts = [seg["text"]]
            
            # Look for nearby segments to extend to min_duration
            for other in scored_segments:
                if other["start"] > clip_end and other["start"] - clip_end < 10:
                    # Adjacent segment
                    clip_end = other["end"]
                    clip_texts.append(other["text"])
                    for t in range(int(other["start"]), int(other["end"]) + 1):
                        used_times.add(t)
            
            clip_duration = clip_end - clip_start
            
            if clip_duration >= min_duration:
                clips.append({
                    "start": clip_start,
                    "end": clip_end,
                    "duration": clip_duration,
                    "text": " ".join(clip_texts),
                    "score": seg["score"]
                })
                
                # Mark these times as used
                for t in range(int(clip_start), int(clip_end) + 1):
                    used_times.add(t)
            
            # Stop if we have enough clips
            if len(clips) >= 5:
                break
        
        return clips
    
    def generate_srt(self, video_path, output_path):
        """Generate SRT subtitle file"""
        result = self.transcribe(video_path)
        segments = result["segments"]
        
        with open(output_path, 'w', encoding='utf-8') as srt_file:
            for i, seg in enumerate(segments, start=1):
                start_time = str(timedelta(seconds=seg["start"]))
                end_time = str(timedelta(seconds=seg["end"]))
                
                # Format time for SRT (remove milliseconds, add comma)
                start_time = start_time.split('.')[0].replace('-', '') + ',000'
                end_time = end_time.split('.')[0].replace('-', '') + ',000'
                
                srt_file.write(f"{i}\n")
                srt_file.write(f"{start_time} --> {end_time}\n")
                srt_file.write(f"{seg['text'].strip()}\n\n")
from app.celery_app import celery
from app.tasks.whisper_processor import WhisperProcessor
import os
import json

# Initialize Whisper once (loads model into memory)
whisper = WhisperProcessor(model_size="base")

@celery.task(bind=True)
def process_video(self, video_path: str):
    """Process video with Whisper AI to generate clips and captions"""
    
    try:
        # Update status
        self.update_state(state='PROCESSING', meta={'stage': 'transcribing with Whisper'})
        
        # Step 1: Get transcription and highlights
        highlights = whisper.extract_highlights(
            video_path, 
            min_duration=60,  # Minimum 60-second clips
            max_duration=120  # Maximum 120-second clips
        )
        
        self.update_state(state='PROCESSING', meta={'stage': 'generating captions'})
        
        # Step 2: Generate SRT file
        video_id = os.path.basename(video_path).split('.')[0]
        captions_dir = "./captions"
        os.makedirs(captions_dir, exist_ok=True)
        
        srt_path = os.path.join(captions_dir, f"{video_id}.srt")
        whisper.generate_srt(video_path, srt_path)
        
        self.update_state(state='PROCESSING', meta={'stage': 'analyzing highlights'})
        
        # Step 3: Prepare clip data with AI scores
        clips = []
        for i, highlight in enumerate(highlights):
            clip_data = {
                "id": i + 1,
                "start_time": highlight["start"],
                "end_time": highlight["end"],
                "duration": highlight["duration"],
                "ai_score": highlight["score"],
                "text_snippet": highlight["text"][:200] + "...",
                "thumbnail": f"thumb_{video_id}_{i+1}.jpg"  # Will be generated later
            }
            clips.append(clip_data)
        
        # If no highlights found, create placeholder
        if not clips:
            clips = [{
                "id": 1,
                "start_time": 0,
                "end_time": 60,
                "duration": 60,
                "ai_score": 50,
                "text_snippet": "Automatic clip (no highlights detected)",
                "thumbnail": f"thumb_{video_id}_1.jpg"
            }]
        
        # Sort clips by AI score (highest first)
        clips.sort(key=lambda x: x["ai_score"], reverse=True)
        
        return {
            "status": "completed",
            "video_id": video_id,
            "video_path": video_path,
            "captions_file": srt_path,
            "clips": clips,
            "total_clips": len(clips),
            "message": "Whisper AI processing completed successfully"
        }
        
    except Exception as e:
        return {
            "status": "failed",
            "error": str(e),
            "message": "Processing failed"
        }
FROM python:3.11-slim

# Install system dependencies for audio processing
RUN apt-get update && apt-get install -y \
    ffmpeg \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY app ./app

# Create directories for outputs
RUN mkdir -p /app/uploads /app/clips /app/thumbnails /app/captions

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
