fastapi
uvicorn
celery
redis
python-dotenv
python-multipart
aiofiles
openai-whisper
torch
torchaudio
numpy
opencv-python-headless
pillow
ffmpeg-python
openai
elevenlabs
boto3
replicate              # NEW - For Stable Diffusion/Runway
pillow                 # Already have
import os
import json
import openai
from typing import List, Dict
import subprocess

class AdvancedVoiceoverGenerator:
    """
    Creates different voiceover styles:
    - Solo narrator (existing)
    - Dual host conversation (like NotebookLM)
    - Interview style (Q&A)
    - Debate format (pro/con)
    - Storytelling mode
    """
    
    def __init__(self):
        self.openai_key = os.getenv("OPENAI_API_KEY")
        self.elevenlabs_key = os.getenv("ELEVENLABS_API_KEY")
        openai.api_key = self.openai_key
    
    def generate_script(self, transcript_snippet: str, style: str) -> Dict:
        """
        Generate script in different styles
        Returns script with speaker assignments
        """
        
        style_prompts = {
            "solo": """
                Write a solo narrator script that's engaging and informative.
                Speaker: Single host
                Tone: Energetic and clear
                Length: 30-60 seconds
            """,
            
            "dual_host": """
                Write a script for TWO AI hosts having a natural conversation about this content.
                
                Host 1 (Alex): Enthusiastic, asks questions, reacts with excitement
                Host 2 (Jordan): Analytical, provides context, explains concepts
                
                Make it sound like a podcast - natural banter, reactions, back-and-forth.
                Include:
                - Opening banter/introduction
                - Discussion of key points
                - Reactions ("Wow!", "That's amazing!", "Wait, really?")
                - Closing thoughts
                
                Format as: [HOST1]: text | [HOST2]: text
            """,
            
            "interview": """
                Write an interview-style script.
                
                Host: Asks insightful questions, guides conversation
                Expert: Provides answers, explains concepts
                
                Make it sound natural with:
                - Host introduces topic
                - Host asks 3-4 key questions
                - Expert gives detailed answers
                - Host summarizes at end
                
                Format as: [HOST]: text | [EXPERT]: text
            """,
            
            "debate": """
                Write a debate-style script with two perspectives.
                
                Speaker A (Pro): Arguments FOR the main points
                Speaker B (Con): Counter-arguments, alternative views
                Moderator: Introduces topic, manages flow (optional)
                
                Make it engaging with:
                - Opening statements
                - Rebuttals
                - Closing arguments
                
                Format as: [PRO]: text | [CON]: text | [MOD]: text
            """,
            
            "storytelling": """
                Write a storytelling/narrative script.
                
                Narrator: Dramatic, engaging storyteller
                Sound effects: [SFX] markers for ambient sounds
                
                Make it cinematic with:
                - Hook opening
                - Rising tension
                - Climax/reveal
                - Satisfying conclusion
                
                Format as: [NARRATOR]: text with [SFX] markers
            """
        }
        
        prompt = f"""
        {style_prompts.get(style, style_prompts["solo"])}
        
        Based on this transcript from a video clip, create a {style} voiceover script.
        
        Original content: "{transcript_snippet}"
        
        Return as JSON with:
        - script: formatted with speaker markers
        - speakers: list of speaker names
        - duration_estimate: seconds
        - tone: overall tone description
        """
        
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",  # Using GPT-4 for better conversation quality
                messages=[
                    {"role": "system", "content": "You are an expert scriptwriter for podcasts and videos."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.8,
                max_tokens=800
            )
            
            result = json.loads(response.choices[0].message.content)
            return result
            
        except Exception as e:
            # Fallback script
            return {
                "script": f"[HOST1]: Let's talk about this interesting clip. | [HOST2]: I agree, there's so much to discuss!",
                "speakers": ["HOST1", "HOST2"],
                "duration_estimate": 30,
                "tone": "conversational"
            }
    
    def parse_script(self, script_text: str):
        """Parse script with [SPEAKER]: text format"""
        import re
        lines = script_text.split('|')
        parsed = []
        
        for line in lines:
            match = re.match(r'\[(.*?)\]:\s*(.*)', line.strip())
            if match:
                speaker, text = match.groups()
                parsed.append({
                    "speaker": speaker.strip(),
                    "text": text.strip()
                })
        
        return parsed
    
    def generate_multi_speaker_audio(self, parsed_script: List[Dict], output_path: str) -> bool:
        """
        Generate audio with multiple speakers using different voices
        """
        try:
            import tempfile
            from pydub import AudioSegment
            
            audio_segments = []
            speaker_voices = {
                "HOST1": "21m00Tcm4TlvDq8ikWAM",  # Rachel
                "HOST2": "AZnzlk1XvdvUeBnXmlld",  # Domi
                "HOST": "EXAVITQu4vr4xnSDxMaL",   # Sarah
                "EXPERT": "TxGEqnHWrfWFTfGW9XjX", # Josh
                "PRO": "pNInz6obpgDQGcFmaJgB",    # Adam
                "CON": "yoZ06aMxZJJ28mfd3POQ",    # Sam
                "MOD": "XrExE9yKIg1WjnnlVkGX",    # Emily
                "NARRATOR": "MF3mGyEYCl7XYWbV9V6O" # Elli
            }
            
            for segment in parsed_script:
                speaker = segment["speaker"]
                text = segment["text"]
                
                # Choose voice based on speaker
                voice_id = speaker_voices.get(speaker, "21m00Tcm4TlvDq8ikWAM")
                
                # Generate audio for this segment
                temp_file = tempfile.NamedTemporaryFile(suffix='.mp3', delete=False)
                
                from elevenlabs import generate
                audio = generate(
                    text=text,
                    voice=voice_id,
                    model="eleven_monolingual_v1"
                )
                
                with open(temp_file.name, "wb") as f:
                    f.write(audio)
                
                audio_segments.append(AudioSegment.from_mp3(temp_file.name))
                
                # Add small pause between speakers
                if len(parsed_script) > 1:
                    pause = AudioSegment.silent(duration=500)
                    audio_segments.append(pause)
            
            # Combine all segments
            combined = AudioSegment.empty()
            for segment in audio_segments:
                combined += segment
            
            # Export
            combined.export(output_path, format="mp3")
            return True
            
        except Exception as e:
            print(f"Multi-speaker audio failed: {e}")
            return False
    
    def generate_voiceover(self, transcript_snippet: str, style: str, output_path: str) -> Dict:
        """
        Complete pipeline: script generation -> multi-speaker audio
        """
        # Step 1: Generate script in requested style
        script_data = self.generate_script(transcript_snippet, style)
        
        # Step 2: Parse script into speaker segments
        parsed = self.parse_script(script_data['script'])
        
        # Step 3: Generate multi-speaker audio
        success = self.generate_multi_speaker_audio(parsed, output_path)
        
        if success:
            return {
                "success": True,
                "style": style,
                "script": script_data['script'],
                "speakers": script_data['speakers'],
                "audio_path": output_path,
                "duration_estimate": script_data['duration_estimate']
            }
        else:
            return {"success": False, "error": "Audio generation failed"}
import os
import openai
import replicate
from PIL import Image, ImageDraw, ImageFont, ImageFilter
import requests
from io import BytesIO

class ThumbnailStylist:
    """
    Applies different visual styles to thumbnails:
    - Watercolor (artistic, soft)
    - Anime (vibrant, stylized)
    - Whiteboard (clean, educational)
    - Retro Print (vintage, textured)
    - Paper-Craft (layered, handmade)
    - Cinematic (dramatic, movie-style)
    - Clickbait (bright, high-contrast)
    """
    
    def __init__(self):
        self.openai_key = os.getenv("OPENAI_API_KEY")
        self.replicate_token = os.getenv("REPLICATE_API_TOKEN")
        openai.api_key = self.openai_key
    
    def generate_context_description(self, transcript_snippet: str) -> str:
        """Create detailed image prompt from transcript"""
        prompt = f"""
        Create a detailed image description for a YouTube thumbnail based on this transcript.
        
        Transcript: "{transcript_snippet}"
        
        Include:
        - Main subject/theme
        - Key objects/people
        - Mood/emotion
        - Colors that would work well
        
        Return a single paragraph description perfect for AI image generation.
        """
        
        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "You create detailed image descriptions for thumbnails."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=150
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            return f"A scene about: {transcript_snippet[:100]}"
    
    def apply_style_prompt(self, base_description: str, style: str) -> str:
        """Enhance description with style-specific prompts"""
        
        style_prompts = {
            "watercolor": """
                Style: Watercolor painting
                Artistic, soft edges, flowing colors, painterly texture,
                gentle gradients, artistic impression, beautiful washes of color,
                painted on textured paper, professional art style
            """,
            
            "anime": """
                Style: Anime/Manga
                Vibrant colors, cel-shaded, large expressive eyes if people,
                dynamic composition, clean lines, Japanese animation style,
                dramatic lighting, energetic, Studio Ghibli inspired
            """,
            
            "whiteboard": """
                Style: Whiteboard animation
                Clean lines, simple shapes, educational style, hand-drawn look,
                marker on white background, instructional design,
                minimal colors (mostly black, blue, red, green)
            """,
            
            "retro_print": """
                Style: Retro vintage print
                Aged paper texture, faded colors, halftone dots, screen printing look,
                1970s/80s aesthetic, warm tones, slightly distressed,
                classic poster style with character
            """,
            
            "paper_craft": """
                Style: Paper-craft
                Layered paper cutouts, dimensional, shadow depth, handmade feel,
                textured paper, origami elements, craft aesthetic,
                realistic paper shadows and layers
            """,
            
            "cinematic": """
                Style: Cinematic movie poster
                Dramatic lighting, deep shadows, rich colors, epic scale,
                Hollywood production quality, film grain, anamorphic lens look,
                blockbuster movie poster aesthetic
            """,
            
            "clickbait": """
                Style: YouTube clickbait thumbnail
                Ultra bright colors, high contrast, bold text space reserved,
                surprised expressions if people, arrows and highlights,
                eye-catching, maximum click-through rate optimized,
                YouTube algorithm friendly
            """
        }
        
        return f"{base_description}\n\n{style_prompts.get(style, style_prompts['cinematic'])}"
    
    def generate_ai_thumbnail(self, transcript_snippet: str, style: str, output_path: str) -> bool:
        """
        Generate AI-powered thumbnail based on transcript content
        """
        # Step 1: Get context description
        context = self.generate_context_description(transcript_snippet)
        
        # Step 2: Apply style
        full_prompt = self.apply_style_prompt(context, style)
        
        # Step 3: Add text overlay instructions
        full_prompt += "\n\nLeave space at top and bottom for text overlay. Vertical orientation 1280x720."
        
        try:
            # Try DALL-E 3 first
            response = openai.Image.create(
                model="dall-e-3",
                prompt=full_prompt[:1000],  # DALL-E has limit
                size="1792x1024",  # Close to 16:9
                quality="hd",
                n=1
            )
            
            # Download image
            image_url = response.data[0].url
            img_response = requests.get(image_url)
            img = Image.open(BytesIO(img_response.content))
            
            # Resize to standard thumbnail size
            img = img.resize((1280, 720), Image.Resampling.LANCZOS)
            
            # Apply style-specific post-processing
            img = self.apply_style_postprocess(img, style)
            
            # Save
            img.save(output_path, "JPEG", quality=95)
            
            # Also create web version
            web_path = output_path.replace('.jpg', '_web.jpg')
            web_img = img.resize((640, 360), Image.Resampling.LANCZOS)
            web_img.save(web_path, "JPEG", quality=85)
            
            return True
            
        except Exception as e:
            print(f"AI thumbnail generation failed: {e}")
            
            # Fallback to Replicate (Stable Diffusion)
            try:
                import replicate
                
                output = replicate.run(
                    "stability-ai/stable-diffusion:db21e45d3f7023abc2a46ee38a23973f6dce16bb082a930b0c49861f96d1e5bf",
                    input={
                        "prompt": full_prompt,
                        "width": 1280,
                        "height": 720,
                        "num_outputs": 1,
                        "scheduler": "DPMSolverMultistep"
                    }
                )
                
                img_response = requests.get(output[0])
                img = Image.open(BytesIO(img_response.content))
                img.save(output_path, "JPEG", quality=95)
                
                web_path = output_path.replace('.jpg', '_web.jpg')
                web_img = img.resize((640, 360), Image.Resampling.LANCZOS)
                web_img.save(web_path, "JPEG", quality=85)
                
                return True
                
            except Exception as e2:
                print(f"Replicate also failed: {e2}")
                return False
    
    def apply_style_postprocess(self, img: Image, style: str) -> Image:
        """Apply additional post-processing based on style"""
        
        if style == "watercolor":
            # Soft, painterly effect
            img = img.filter(ImageFilter.SMOOTH_MORE)
            
        elif style == "retro_print":
            # Add grain and slight fade
            from PIL import ImageEnhance
            enhancer = ImageEnhance.Color(img)
            img = enhancer.enhance(0.8)
            
        elif style == "cinematic":
            # Add letterbox bars
            draw = ImageDraw.Draw(img)
            draw.rectangle([(0, 0), (1280, 60)], fill="black")
            draw.rectangle([(0, 660), (1280, 720)], fill="black")
            
        elif style == "clickbait":
            # Boost contrast and saturation
            from PIL import ImageEnhance
            contrast = ImageEnhance.Contrast(img)
            img = contrast.enhance(1.2)
            color = ImageEnhance.Color(img)
            img = color.enhance(1.3)
        
        return img
    
    def add_text_overlay(self, thumbnail_path: str, title: str, output_path: str = None):
        """Add title text to thumbnail"""
        try:
            img = Image.open(thumbnail_path)
            draw = ImageDraw.Draw(img)
            
            # Try to load a font, fallback to default
            try:
                font = ImageFont.truetype("Arial Bold.ttf", 60)
                small_font = ImageFont.truetype("Arial.ttf", 30)
            except:
                font = ImageFont.load_default()
                small_font = font
            
            # Add gradient overlay for text
            overlay = Image.new('RGBA', img.size, (0,0,0,0))
            overlay_draw = ImageDraw.Draw(overlay)
            
            # Semi-transparent bar at bottom
            overlay_draw.rectangle(
                [(0, 520), (1280, 720)], 
                fill=(0, 0, 0, 180)
            )
            
            img = Image.alpha_composite(img.convert('RGBA'), overlay)
            draw = ImageDraw.Draw(img)
            
            # Add title text
            words = title.split()
            line1 = " ".join(words[:4])
            line2 = " ".join(words[4:8]) if len(words) > 4 else ""
            
            # Text with stroke effect
            text_color = (255, 255, 255)
            stroke_color = (0, 0, 0)
            
            # Draw text twice for stroke effect
            for offset in [(2,2), (-2,-2), (2,-2), (-2,2), (0,0)]:
                x_offset, y_offset = offset
                if offset == (0,0):
                    draw_color = text_color
                else:
                    draw_color = stroke_color
                
                draw.text((100 + x_offset, 560 + y_offset), line1, 
                         font=font, fill=draw_color)
                if line2:
                    draw.text((100 + x_offset, 630 + y_offset), line2, 
                             font=small_font, fill=draw_color)
            
            # Save
            if output_path:
                img.save(output_path, "JPEG", quality=95)
            else:
                img.save(thumbnail_path, "JPEG", quality=95)
            
            return True
            
        except Exception as e:
            print(f"Text overlay failed: {e}")
            return False
import subprocess
import os

class MultiLengthClipProcessor:
    """
    Creates clips in multiple lengths:
    - Teaser: 15-30 seconds (TikTok/Reels)
    - Standard: 60 seconds (default)
    - Explainer: 120-180 seconds (YouTube)
    """
    
    def __init__(self, output_dir="./clips"):
        self.output_dir = output_dir
    
    def extract_teaser(self, video_path, start_time, end_time, output_path, clip_id):
        """Extract short teaser (15-30 seconds)"""
        try:
            duration = min(end_time - start_time, 30)
            
            # Find the most engaging 15-second segment within the clip
            # Usually the middle or where AI score peaks
            peak_time = start_time + (duration / 2)
            
            cmd = [
                'ffmpeg', '-i', video_path,
                '-ss', str(peak_time - 7.5),  # 7.5 seconds before peak
                '-t', '15',
                '-c:v', 'libx264', '-c:a', 'aac',
                '-vf', 'scale=720:1280:force_original_aspect_ratio=increase,crop=720:1280',
                '-movflags', '+faststart',
                '-y', output_path
            ]
            
            subprocess.run(cmd, check=True, capture_output=True)
            
            return {
                "success": True,
                "path": output_path,
                "duration": 15,
                "type": "teaser"
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def extract_standard(self, video_path, start_time, end_time, output_path):
        """Extract standard 60-second clip"""
        try:
            duration = min(end_time - start_time, 60)
            
            cmd = [
                'ffmpeg', '-i', video_path,
                '-ss', str(start_time),
                '-t', str(duration),
                '-c:v', 'libx264', '-c:a', 'aac',
                '-b:v', '2000k', '-b:a', '128k',
                '-movflags', '+faststart',
                '-y', output_path
            ]
            
            subprocess.run(cmd, check=True, capture_output=True)
            
            return {
                "success": True,
                "path": output_path,
                "duration": duration,
                "type": "standard"
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def extract_explainer(self, video_path, start_time, end_time, output_path, transcript_snippet):
        """Extract longer explainer (2-3 minutes) with more context"""
        try:
            # Extend to include more context before/after
            extended_start = max(0, start_time - 30)
            extended_end = min(end_time + 30, end_time + 90)  # Add up to 90 seconds
            duration = extended_end - extended_start
            
            # Cap at 3 minutes
            if duration > 180:
                duration = 180
                extended_end = extended_start + 180
            
            cmd = [
                'ffmpeg', '-i', video_path,
                '-ss', str(extended_start),
                '-t', str(duration),
                '-c:v', 'libx264', '-c:a', 'aac',
                '-b:v', '2500k',  # Higher quality for longer content
                '-b:a', '160k',
                '-movflags', '+faststart',
                '-y', output_path
            ]
            
            subprocess.run(cmd, check=True, capture_output=True)
            
            return {
                "success": True,
                "path": output_path,
                "duration": duration,
                "type": "explainer",
                "start": extended_start,
                "end": extended_end
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def extract_all_lengths(self, video_path, clip_data, video_id):
        """Extract all three clip lengths for each highlight"""
        output_dir = os.path.join(self.output_dir, video_id)
        os.makedirs(output_dir, exist_ok=True)
        
        results = []
        
        for clip in clip_data:
            clip_id = clip['id']
            start = clip['start_time']
            end = clip['end_time']
            transcript = clip.get('text_snippet', '')
            
            clip_results = {
                "clip_id": clip_id,
                "lengths": {}
            }
            
            # Teaser (15-30 sec)
            teaser_path = os.path.join(output_dir, f"clip_{clip_id:03d}_teaser.mp4")
            teaser = self.extract_teaser(video_path, start, end, teaser_path, clip_id)
            if teaser['success']:
                clip_results['lengths']['teaser'] = teaser
            
            # Standard (60 sec)
            standard_path = os.path.join(output_dir, f"clip_{clip_id:03d}_standard.mp4")
            standard = self.extract_standard(video_path, start, end, standard_path)
            if standard['success']:
                clip_results['lengths']['standard'] = standard
            
            # Explainer (2-3 min)
            explainer_path = os.path.join(output_dir, f"clip_{clip_id:03d}_explainer.mp4")
            explainer = self.extract_explainer(video_path, start, end, explainer_path, transcript)
            if explainer['success']:
                clip_results['lengths']['explainer'] = explainer
            
            results.append(clip_results)
        
        return results
# Add these imports at the top
from app.tasks.voiceover_styles import AdvancedVoiceoverGenerator
from app.tasks.thumbnail_styles import ThumbnailStylist
from app.tasks.clip_lengths import MultiLengthClipProcessor

# Initialize new processors
advanced_voiceover = AdvancedVoiceoverGenerator()
thumbnail_stylist = ThumbnailStylist()
multi_length_clips = MultiLengthClipProcessor()

# In the process_video function, add these stages:

# === STAGE: MULTI-LENGTH CLIPS ===
self.update_state(state='PROCESSING', meta={'stage': 'creating multiple clip lengths'})

clip_lengths = multi_length_clips.extract_all_lengths(
    video_path,
    clip_data,
    video_id
)

# === STAGE: STYLED VOICEOVERS ===
self.update_state(state='PROCESSING', meta={'stage': 'generating advanced voiceovers'})

voiceover_results = []
voiceover_styles = ["solo", "dual_host", "interview", "debate", "storytelling"]

for i, clip in enumerate(clip_lengths[:2]):  # Do for top 2 clips
    for style in voiceover_styles[:2]:  # Do 2 styles per clip
        audio_path = f"./voiceovers/{video_id}/clip_{i+1}_{style}.mp3"
        os.makedirs(f"./voiceovers/{video_id}", exist_ok=True)
        
        result = advanced_voiceover.generate_voiceover(
            highlights[i]['text'],
            style,
            audio_path
        )
        
        if result['success']:
            voiceover_results.append({
                'clip_id': i + 1,
                'style': style,
                'audio_path': audio_path,
                'script': result['script']
            })

# === STAGE: STYLED THUMBNAILS ===
self.update_state(state='PROCESSING', meta={'stage': 'generating AI thumbnails'})

thumbnail_styles = ["cinematic", "anime", "watercolor", "retro_print", "whiteboard", "clickbait"]
styled_thumbnails = []

for i, clip in enumerate(clip_lengths):
    for style in thumbnail_styles:
        thumb_path = f"./thumbnails/{video_id}/clip_{i+1}_{style}.jpg"
        
        success = thumbnail_stylist.generate_ai_thumbnail(
            highlights[i]['text'],
            style,
            thumb_path
        )
        
        if success:
            # Add text overlay with clip title
            title = clip_titles[i]['best_title'] if i < len(clip_titles) else f"Clip {i+1}"
            thumbnail_stylist.add_text_overlay(thumb_path, title)
            
            styled_thumbnails.append({
                'clip_id': i + 1,
                'style': style,
                'path': thumb_path,
                'web_path': thumb_path.replace('.jpg', '_web.jpg')
            })
// Voiceover style selector
{clip.voiceover_options && clip.voiceover_options.length > 0 && (
  <div className="mt-4">
    <label className="text-xs text-gray-500 mb-2 block">Voiceover Styles</label>
    <div className="flex flex-wrap gap-2">
      {clip.voiceover_options.map((vo, idx) => (
        <button
          key={idx}
          onClick={() => setSelectedVoiceover(vo)}
          className="px-3 py-1 bg-purple-50 text-purple-700 rounded-full text-xs hover:bg-purple-100"
        >
          {vo.style === 'dual_host' && 'üéôÔ∏è Dual Host'}
          {vo.style === 'interview' && 'üé§ Interview'}
          {vo.style === 'debate' && '‚öñÔ∏è Debate'}
          {vo.style === 'storytelling' && 'üìñ Story'}
          {vo.style === 'solo' && 'üéß Solo'}
        </button>
      ))}
    </div>
  </div>
)}

// Thumbnail style selector
{clip.styled_thumbnails && clip.styled_thumbnails.length > 0 && (
  <div className="mt-4">
    <label className="text-xs text-gray-500 mb-2 block">Thumbnail Styles</label>
    <div className="grid grid-cols-3 gap-2">
      {clip.styled_thumbnails.map((thumb, idx) => (
        <button
          key={idx}
          onClick={() => setSelectedThumbnail(thumb)}
          className="relative aspect-video rounded-lg overflow-hidden border-2 hover:border-primary transition-colors"
        >
          <img 
            src={`http://localhost:8000/${thumb.web_path}`}
            alt={thumb.style}
            className="w-full h-full object-cover"
          />
          <span className="absolute bottom-0 left-0 right-0 bg-black bg-opacity-50 text-white text-xs p-1">
            {thumb.style}
          </span>
        </button>
      ))}
    </div>
  </div>
)}

// Clip length selector
{clip.multi_length && (
  <div className="mt-4 flex gap-2">
    <button className="flex-1 px-2 py-1 bg-blue-50 text-blue-700 rounded text-sm hover:bg-blue-100">
      üî• Teaser (15s)
    </button>
    <button className="flex-1 px-2 py-1 bg-green-50 text-green-700 rounded text-sm hover:bg-green-100">
      üì∫ Standard (60s)
    </button>
    <button className="flex-1 px-2 py-1 bg-orange-50 text-orange-700 rounded text-sm hover:bg-orange-100">
      üé¨ Explainer (2-3m)
    </button>
  </div>
)}
# API Keys for AI features
OPENAI_API_KEY=your_openai_api_key_here
ELEVENLABS_API_KEY=your_elevenlabs_api_key_here
REPLICATE_API_TOKEN=your_replicate_token_here

# Redis configuration
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/0
